{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert for Question-Paragraph classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AsmaTidafi/Bert-keras-implementation/blob/main/Bert_for_Question_Paragraph_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy1NmDv0GzEp"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qJ4-d6H1sNH"
      },
      "source": [
        "!pip install tensorflow\n",
        "!pip install tokenizers\n",
        "!pip install transformers\n",
        "\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from transformers import BertConfig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF8aDD_Wrtxq"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knKKGvNy3ta2"
      },
      "source": [
        "df_train = pd.read_csv('train.tsv', delimiter='\\t', names=['index', 'question', 'sentence', 'label'])\n",
        "df_train.set_index('index', inplace=True)\n",
        "\n",
        "df_test = pd.read_csv('dev.tsv', delimiter='\\t', names=['index', 'question', 'sentence', 'label'])\n",
        "df_test.set_index('index', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi7eEhJJ_m8I"
      },
      "source": [
        "df_train = df_train.drop([df_train.index[0]])\n",
        "df_test = df_test.drop([df_test.index[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utR1qDbiBmyP"
      },
      "source": [
        "df_test = df_test[df_test.label.str.contains('entailment')]\n",
        "df_train = df_train[df_train.label.str.contains('entailment')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgtdJdqRCEpz"
      },
      "source": [
        "possible_labels = df_train.label.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WHS5uT3BTf-"
      },
      "source": [
        "label_dict = {}\n",
        "for index, possible_label in enumerate(possible_labels):\n",
        "    label_dict[possible_label] = index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJkrlBslBX8d"
      },
      "source": [
        "label_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0wP8StTB9k7"
      },
      "source": [
        "df_train['label'] = df_train.label.replace(label_dict)\n",
        "df_test['label'] = df_test.label.replace(label_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1agXaCaWBGX"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, val = train_test_split(df_train, test_size=0.2, random_state=0)\n",
        "df_train = pd.DataFrame(train)\n",
        "df_val = pd.DataFrame(val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvLqiobXXgzX"
      },
      "source": [
        "len(df_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQgGnufHXkBZ"
      },
      "source": [
        "len(df_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa8lAFcOrJxr"
      },
      "source": [
        "## Set up tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltT_uX4jDfzv"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    do_lower_case=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYyYnDU6DB_M"
      },
      "source": [
        "max_len = 300\n",
        "class SquadExample:\n",
        "    def __init__(self, question, context, is_impossible):\n",
        "        self.question = question\n",
        "        self.context = context\n",
        "        self.is_impossible = is_impossible\n",
        "\n",
        "    def preprocess(self):\n",
        "        context = self.context\n",
        "        question = self.question\n",
        "        is_impossible = self.is_impossible\n",
        "\n",
        "        # Clean context, answer and question\n",
        "        context = \" \".join(str(context).split())\n",
        "        question = \" \".join(str(question).split())\n",
        "\n",
        "        # Tokenize context\n",
        "        tokenized_context = tokenizer.encode(context)\n",
        "\n",
        "        # Tokenize question\n",
        "        tokenized_question = tokenizer.encode(question)\n",
        "\n",
        "        # Create inputs\n",
        "        input_ids = tokenized_context + tokenized_question[1:]\n",
        "        token_type_ids = [0] * len(tokenized_context) + [1] * len(tokenized_question[1:])\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Pad and create attention masks.\n",
        "        # Skip if truncation is needed\n",
        "        padding_length = max_len - len(input_ids)\n",
        "        if padding_length > 0:  # pad\n",
        "            input_ids = input_ids + ([0] * padding_length)\n",
        "            attention_mask = attention_mask + ([0] * padding_length)\n",
        "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "        elif padding_length < 0:  # skip\n",
        "            tokenized_context_ = []\n",
        "            m = len(tokenized_context) + padding_length - 1\n",
        "            \n",
        "            i = 0\n",
        "            for item in tokenized_context:\n",
        "              if i > m:\n",
        "                break\n",
        "              else:\n",
        "                tokenized_context_.append(item)\n",
        "                i += 1\n",
        "            \n",
        "            input_ids = tokenized_context_ + tokenized_question[1:]\n",
        "            token_type_ids = [0] * len(tokenized_context_) + [1] * len(tokenized_question[1:])\n",
        "            attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        self.input_ids = input_ids\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.is_impossible = is_impossible\n",
        "\n",
        "\n",
        "def create_squad_examples(df):\n",
        "    squad_examples = []\n",
        "    for index, row in df.iterrows():\n",
        "        question = str(row['question'])\n",
        "        sentence = str(row['sentence'])\n",
        "        label = row['label']\n",
        "        squad_eg = SquadExample(question, sentence, label)\n",
        "        squad_eg.preprocess()\n",
        "        squad_examples.append(squad_eg)\n",
        "    return squad_examples\n",
        "\n",
        "\n",
        "def create_inputs_targets(squad_examples):\n",
        "    dataset_dict = {\n",
        "        \"input_ids\": [],\n",
        "        \"token_type_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"is_impossible\": [],\n",
        "    }\n",
        "    for item in squad_examples:\n",
        "        for key in dataset_dict:\n",
        "            dataset_dict[key].append(getattr(item, key))\n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "\n",
        "    x = [\n",
        "        dataset_dict[\"input_ids\"],\n",
        "        dataset_dict[\"token_type_ids\"],\n",
        "        dataset_dict[\"attention_mask\"],\n",
        "    ]\n",
        "    y = [dataset_dict[\"is_impossible\"]]\n",
        "\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqiOyDMXrO3B"
      },
      "source": [
        "## Tokenize data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9NZOkIzDcNz"
      },
      "source": [
        "train_squad_examples = create_squad_examples(df_train)\n",
        "val_squad_examples = create_squad_examples(df_val)\n",
        "test_squad_examples = create_squad_examples(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW-jRhDVFT38"
      },
      "source": [
        "x_train, y_train = create_inputs_targets(train_squad_examples)\n",
        "print(f\"{len(train_squad_examples)} training points created.\")\n",
        "\n",
        "x_val, y_val = create_inputs_targets(val_squad_examples)\n",
        "print(f\"{len(val_squad_examples)} val points created.\")\n",
        "\n",
        "x_test, y_test = create_inputs_targets(test_squad_examples)\n",
        "print(f\"{len(test_squad_examples)} test points created.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW4p9BRprTLp"
      },
      "source": [
        "## Create model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOPpgXnmH6H7"
      },
      "source": [
        "def create_model(dropout_value):\n",
        "    config = BertConfig(hidden_dropout_prob=dropout_value)\n",
        "    ## BERT encoder\n",
        "    encoder = TFBertModel.from_pretrained(\"bert-base-uncased\", config=config)\n",
        "    \n",
        "    # QA Model\n",
        "    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    output = encoder(\n",
        "        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
        "    )[1]\n",
        "    \n",
        "    output = layers.Dense(1, use_bias=True)(output)\n",
        "    output = layers.Activation(keras.activations.sigmoid)(output)\n",
        "\n",
        "\n",
        "    model = keras.Model(\n",
        "        inputs=[input_ids, token_type_ids, attention_mask],\n",
        "        outputs=output,\n",
        "    )\n",
        "\n",
        "    # creates an optimizer with learning rate schedule\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=1e-5)\n",
        "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)\n",
        "               tf.keras.metrics.TruePositives(),\n",
        "               tf.keras.metrics.TrueNegatives(),\n",
        "               tf.keras.metrics.FalsePositives(),\n",
        "               tf.keras.metrics.FalseNegatives()]\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=loss,\n",
        "        metrics=metrics)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe6Ag0uZrWDu"
      },
      "source": [
        "## Use TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_-ctM12IMKY"
      },
      "source": [
        "# Create distribution strategy\n",
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf_JJL_TrbGD"
      },
      "source": [
        "## Run training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs4X8mBmIcUV"
      },
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "# Create model\n",
        "with strategy.scope():\n",
        "    model = create_model(0.2)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "output = model.fit(x_train,\n",
        "                  y_train,\n",
        "                  epochs=100,\n",
        "                  verbose=1,\n",
        "                  batch_size=128,\n",
        "                  validation_data=(x_val, y_val),\n",
        "                  callbacks = callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK9h4eoVrgW0"
      },
      "source": [
        "## Plot graphics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO1uqkDsYc3w"
      },
      "source": [
        "plot([x for x in range(1,len(output.history['val_loss'])+1)], [x for x in output.history['val_loss']], label=\"Validation set\", color=\"g\")\n",
        "plot([x for x in range(1,len(output.history['val_loss'])+1)], [x for x in output.history['loss']], label=\"Training set\", color=\"r\")\n",
        "ylabel('Loss')\n",
        "xlabel('Epochs')\n",
        "legend(loc='best')\n",
        "show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlGm3TbJnumw"
      },
      "source": [
        "plot([x for x in range(1,len(output.history['val_loss'])+1)], [x for x in output.history['val_accuracy']], label=\"Validation set\", color=\"g\")\n",
        "plot([x for x in range(1,len(output.history['val_loss'])+1)], [x for x in output.history['accuracy']], label=\"Training set\", color=\"r\")\n",
        "ylabel('Accuracy')\n",
        "xlabel('Epochs')\n",
        "legend(loc='best')\n",
        "show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjirjxGjoGoM"
      },
      "source": [
        "## Save the model\n",
        "model.save_weights('/content/drive/My Drive/Model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy9-UdM_rnYI"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NlqiNxjxOOP"
      },
      "source": [
        "y_pred = model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-Ffsev4HXJl"
      },
      "source": [
        "y = []\n",
        "for i in y_pred:\n",
        "  if i < 0.5:\n",
        "    y.append(0)\n",
        "  else:\n",
        "    y.append(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoS0eKwcHpzm"
      },
      "source": [
        "sklearn.metrics.accuracy_score(y_test[0], y)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}